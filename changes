Check Model's Predictions: It would be helpful to see what the model is actually predicting. For this, you can print out some of the model's predictions and compare them with the actual labels. This will help us understand whether the model is only predicting one class or if there's an issue with the evaluation metrics.

Try Different Initialization: Sometimes, the model could get stuck in a bad local minimum during training. You could try different initialization methods for your model's parameters.

Try a Simpler Model: Sometimes, simpler models can perform better due to the bias-variance trade-off. You could try a simpler model, such as logistic regression or a simple feed-forward neural network, to see if it performs better.

Analyze Data Further: It could be helpful to further analyze the data to ensure that there are no issues. For example, you could examine the distribution of labels in the training and validation sets to ensure they are similar.

Try Different Loss Function: You might want to try using a different loss function. For instance, you could try using multi-label soft margin loss if you aren't already.